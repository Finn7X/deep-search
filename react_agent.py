"""
ReActä»£ç†æ¡†æ¶
å®ç° Reasoning + Acting çš„å¾ªç¯ï¼šè§„åˆ’ -> æ‰§è¡Œ -> è§‚å¯Ÿ -> åæ€
"""

import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.live import Live
from rich.text import Text

console = Console()

class AgentState(Enum):
    REASONING = "reasoning"     # æ¨ç†åˆ†æé˜¶æ®µ
    PLANNING = "planning"       # åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’
    ACTING = "acting"          # æ‰§è¡Œæœç´¢è¡ŒåŠ¨
    OBSERVING = "observing"    # è§‚å¯Ÿæœç´¢ç»“æœ
    REFLECTING = "reflecting"  # åæ€å’Œè¯„ä¼°
    CONCLUDED = "concluded"    # å¾—å‡ºç»“è®º

@dataclass
class AgentAction:
    action_type: str           # search, analyze, synthesize, conclude
    parameters: Dict[str, Any]
    reasoning: str
    expected_outcome: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class AgentObservation:
    action: AgentAction
    results: Dict[str, Any]
    success: bool
    insights: List[str]
    new_questions: List[str]
    confidence_score: float    # 0-1, å¯¹ç»“æœçš„ä¿¡å¿ƒç¨‹åº¦
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class AgentReflection:
    observations: List[AgentObservation]
    summary: str
    knowledge_gaps: List[str]
    next_actions: List[str]
    should_continue: bool
    overall_progress: float    # 0-1, æ•´ä½“è¿›åº¦
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

class ReActAgent:
    def __init__(self, deepseek_client, tavily_searcher):
        self.deepseek_client = deepseek_client
        self.tavily_searcher = tavily_searcher
        self.state = AgentState.REASONING
        
        # ä»£ç†è®°å¿†
        self.memory = {
            "original_query": "",
            "accumulated_knowledge": "",
            "action_history": [],
            "observation_history": [],
            "reflection_history": [],
            "current_round": 0,
            "max_rounds": 5
        }
        
        # é…ç½®å‚æ•°
        self.config = {
            "max_search_rounds": 5,
            "confidence_threshold": 0.8,
            "knowledge_sufficiency_threshold": 0.7,
            "max_tokens_per_reasoning": 2000
        }
    
    def execute_deep_search(self, original_query: str, query_analysis: Any, 
                          search_plan: Any) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ·±åº¦æœç´¢ReActå¾ªç¯
        """
        console.print(Panel(
            f"[bold magenta]ğŸ¤– å¯åŠ¨ReActä»£ç†[/bold magenta]\n"
            f"[cyan]åŸå§‹æŸ¥è¯¢:[/cyan] {original_query}\n"
            f"[cyan]æœç´¢ç­–ç•¥:[/cyan] {search_plan.strategy.value}\n"
            f"[cyan]æœ€å¤§è½®æ¬¡:[/cyan] {self.config['max_search_rounds']}",
            title="ReAct Deep Search å¼€å§‹",
            border_style="magenta"
        ))
        
        # åˆå§‹åŒ–ä»£ç†è®°å¿†
        self.memory["original_query"] = original_query
        self.memory["query_analysis"] = query_analysis
        self.memory["search_plan"] = search_plan
        self.memory["start_time"] = datetime.now().isoformat()
        
        # å¼€å§‹ReActå¾ªç¯
        final_result = self._react_loop()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self._generate_final_report(final_result)
        
        console.print(Panel(
            f"[bold green]âœ… ReActæœç´¢å®Œæˆ[/bold green]\n"
            f"[cyan]æ€»è½®æ¬¡:[/cyan] {self.memory['current_round']}\n"
            f"[cyan]æœ€ç»ˆçŠ¶æ€:[/cyan] {self.state.value}\n"
            f"[cyan]çŸ¥è¯†ç§¯ç´¯:[/cyan] {len(self.memory['accumulated_knowledge'])} å­—ç¬¦",
            title="ReAct å®Œæˆæ€»ç»“",
            border_style="green"
        ))
        
        return final_report
    
    def _react_loop(self) -> Dict[str, Any]:
        """
        æ ¸å¿ƒReActå¾ªç¯ï¼šReasoning + Acting
        """
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console,
            expand=True
        ) as progress:
            
            task = progress.add_task("ReAct Deep Search è¿›è¡Œä¸­...", total=self.config["max_search_rounds"])
            
            while (self.memory["current_round"] < self.config["max_search_rounds"] and 
                   self.state != AgentState.CONCLUDED):
                
                self.memory["current_round"] += 1
                round_num = self.memory["current_round"]
                
                progress.update(task, description=f"ç¬¬ {round_num} è½® ReAct å¾ªç¯", advance=1)
                
                console.print(f"\n{'='*60}")
                console.print(f"[bold yellow]ğŸ”„ ç¬¬ {round_num} è½® ReAct å¾ªç¯[/bold yellow]")
                console.print(f"{'='*60}")
                
                # 1. Reasoning: æ¨ç†å’Œè§„åˆ’
                self.state = AgentState.REASONING
                reasoning_result = self._reasoning_phase()
                
                if not reasoning_result["should_continue"]:
                    self.state = AgentState.CONCLUDED
                    break
                
                # 2. Acting: æ‰§è¡Œè¡ŒåŠ¨
                self.state = AgentState.ACTING
                action = reasoning_result["planned_action"]
                observation = self._acting_phase(action)
                
                # 3. Observing & Reflecting: è§‚å¯Ÿå’Œåæ€
                self.state = AgentState.REFLECTING
                reflection = self._reflecting_phase([observation])
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­
                if not reflection.should_continue or reflection.overall_progress >= 0.9:
                    self.state = AgentState.CONCLUDED
                    break
        
        return self._finalize_results()
    
    def _reasoning_phase(self) -> Dict[str, Any]:
        """
        æ¨ç†é˜¶æ®µï¼šåˆ†æå½“å‰çŠ¶å†µï¼Œè§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨
        """
        console.print(Panel(
            "[bold blue]ğŸ§  æ¨ç†åˆ†æé˜¶æ®µ[/bold blue]",
            title=f"ç¬¬ {self.memory['current_round']} è½® - æ¨ç†",
            border_style="blue"
        ))
        
        # æ„å»ºæ¨ç†æç¤ºè¯
        reasoning_prompt = self._build_reasoning_prompt()
        
        # è°ƒç”¨AIè¿›è¡Œæ¨ç†
        response = self.deepseek_client.chat_completion(
            messages=[{"role": "user", "content": reasoning_prompt}],
            stream=False,
            temperature=0.3,
            max_tokens=self.config["max_tokens_per_reasoning"]
        )
        
        if "error" in response:
            # å¦‚æœAIæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æ¨ç†
            return self._rule_based_reasoning()
        
        try:
            # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            content = response["content"].strip()
            if content.startswith("```json"):
                content = content[7:]  # ç§»é™¤å¼€å¤´çš„ ```json
            if content.endswith("```"):
                content = content[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
            content = content.strip()
            
            reasoning_data = json.loads(content)
            return self._parse_reasoning_response(reasoning_data)
        except (json.JSONDecodeError, KeyError):
            return self._rule_based_reasoning()
    
    def _build_reasoning_prompt(self) -> str:
        """æ„å»ºæ¨ç†æç¤ºè¯"""
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ·±åº¦æœç´¢ReActä»£ç†ã€‚è¯·åˆ†æå½“å‰æƒ…å†µå¹¶è§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

## åŸå§‹æŸ¥è¯¢
{self.memory["original_query"]}

## å½“å‰è½®æ¬¡
ç¬¬ {self.memory["current_round"]} è½® (æœ€å¤§ {self.config["max_search_rounds"]} è½®)

## å·²ç§¯ç´¯çš„çŸ¥è¯†
{self.memory.get("accumulated_knowledge", "æš‚æ— ")}

## å†å²è¡ŒåŠ¨æ‘˜è¦
{self._summarize_action_history()}

## ä¸Šä¸€è½®è§‚å¯Ÿç»“æœ
{self._summarize_recent_observations()}

## ä»»åŠ¡è¦æ±‚
è¯·åˆ†æå½“å‰æƒ…å†µï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢ï¼Œå¦‚æœéœ€è¦ï¼Œè¯·è§„åˆ’ä¸‹ä¸€æ­¥æœ€æœ‰ä»·å€¼çš„æœç´¢è¡ŒåŠ¨ã€‚

è¯·è¿”å›JSONæ ¼å¼çš„æ¨ç†ç»“æœï¼š
{{
    "current_understanding": "å¯¹é—®é¢˜çš„å½“å‰ç†è§£ç¨‹åº¦ (0-1)",
    "knowledge_gaps": ["çŸ¥è¯†ç¼ºå£1", "çŸ¥è¯†ç¼ºå£2"],
    "should_continue": true/false,
    "reasoning": "è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹",
    "planned_action": {{
        "action_type": "search/analyze/synthesize",
        "parameters": {{
            "queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2"],
            "search_depth": "basic/advanced",
            "focus_areas": ["é‡ç‚¹é¢†åŸŸ1", "é¢†åŸŸ2"]
        }},
        "reasoning": "ä¸ºä»€ä¹ˆè¦æ‰§è¡Œè¿™ä¸ªè¡ŒåŠ¨",
        "expected_outcome": "æœŸæœ›è·å¾—ä»€ä¹ˆç»“æœ"
    }}
}}

åˆ¤æ–­æ ‡å‡†ï¼š
1. å¦‚æœå¯¹åŸå§‹æŸ¥è¯¢çš„ç†è§£åº¦ >= 0.8ï¼Œè€ƒè™‘ç»“æŸæœç´¢
2. å¦‚æœå‘ç°é‡è¦çŸ¥è¯†ç¼ºå£ï¼Œç»§ç»­æœç´¢
3. å¦‚æœå·²è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œå¿…é¡»ç»“æŸ
4. ä¼˜å…ˆå¡«è¡¥æœ€å…³é”®çš„çŸ¥è¯†ç¼ºå£
"""
        return prompt
    
    def _parse_reasoning_response(self, reasoning_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£ææ¨ç†å“åº”"""
        
        understanding = reasoning_data.get("current_understanding", 0.5)
        should_continue = reasoning_data.get("should_continue", True)
        
        # è®°å½•æ¨ç†ç»“æœ
        console.print(Panel(
            f"[bold green]âœ… æ¨ç†å®Œæˆ[/bold green]\n"
            f"[cyan]ç†è§£ç¨‹åº¦:[/cyan] {understanding:.1%}\n"
            f"[cyan]æ˜¯å¦ç»§ç»­:[/cyan] {'æ˜¯' if should_continue else 'å¦'}\n"
            f"[cyan]çŸ¥è¯†ç¼ºå£:[/cyan] {len(reasoning_data.get('knowledge_gaps', []))} ä¸ª",
            title="æ¨ç†ç»“æœ",
            border_style="green"
        ))
        
        if reasoning_data.get("knowledge_gaps"):
            console.print(Panel(
                "\\n".join([f"â€¢ {gap}" for gap in reasoning_data["knowledge_gaps"]]),
                title="ğŸ¯ çŸ¥è¯†ç¼ºå£",
                border_style="yellow"
            ))
        
        return reasoning_data
    
    def _rule_based_reasoning(self) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„æ¨ç†ï¼ˆAIæ¨ç†å¤±è´¥æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        
        round_num = self.memory["current_round"]
        
        # ç®€å•è§„åˆ™ï¼šå‰3è½®ç»§ç»­æœç´¢ï¼Œä¹‹åæ ¹æ®çŸ¥è¯†ç§¯ç´¯å†³å®š
        should_continue = round_num <= 3 or len(self.memory["accumulated_knowledge"]) < 1000
        
        return {
            "current_understanding": min(round_num * 0.2, 0.8),
            "knowledge_gaps": [f"ç¬¬{round_num}è½®çŸ¥è¯†è¡¥å……"],
            "should_continue": should_continue,
            "reasoning": "åŸºäºè§„åˆ™çš„æ¨ç†å†³ç­–",
            "planned_action": {
                "action_type": "search",
                "parameters": {
                    "queries": [self.memory["original_query"] + f" ç¬¬{round_num}è½®"],
                    "search_depth": "advanced",
                    "focus_areas": ["åŸºç¡€ä¿¡æ¯"]
                },
                "reasoning": "ç»§ç»­æ”¶é›†ç›¸å…³ä¿¡æ¯",
                "expected_outcome": "è·å¾—æ›´å¤šç›¸å…³çŸ¥è¯†"
            }
        }
    
    def _acting_phase(self, action: Dict[str, Any]) -> AgentObservation:
        """
        æ‰§è¡Œé˜¶æ®µï¼šæ‰§è¡Œè§„åˆ’çš„æœç´¢è¡ŒåŠ¨
        """
        console.print(Panel(
            f"[bold green]ğŸ¯ æ‰§è¡Œæœç´¢è¡ŒåŠ¨[/bold green]\n"
            f"[cyan]è¡ŒåŠ¨ç±»å‹:[/cyan] {action.get('action_type', 'search')}\n"
            f"[cyan]æŸ¥è¯¢æ•°é‡:[/cyan] {len(action.get('parameters', {}).get('queries', []))}",
            title=f"ç¬¬ {self.memory['current_round']} è½® - æ‰§è¡Œ",
            border_style="green"
        ))
        
        agent_action = AgentAction(
            action_type=action.get("action_type", "search"),
            parameters=action.get("parameters", {}),
            reasoning=action.get("reasoning", ""),
            expected_outcome=action.get("expected_outcome", "")
        )
        
        # æ‰§è¡Œæœç´¢
        search_results = []
        queries = action.get("parameters", {}).get("queries", [])
        
        for query in queries:
            console.print(f"[cyan]ğŸ” æœç´¢æŸ¥è¯¢:[/cyan] {query}")
            
            result = self.tavily_searcher.search(
                query=query,
                search_depth=action.get("parameters", {}).get("search_depth", "advanced")
            )
            
            if not result.get("error"):
                search_results.extend(result.get("results", []))
        
        # åˆ†ææœç´¢ç»“æœï¼Œæå–æ´å¯Ÿ
        insights, new_questions, confidence = self._analyze_search_results(search_results, agent_action)
        
        observation = AgentObservation(
            action=agent_action,
            results={"search_results": search_results, "total_results": len(search_results)},
            success=len(search_results) > 0,
            insights=insights,
            new_questions=new_questions,
            confidence_score=confidence
        )
        
        # æ›´æ–°è®°å¿†
        self.memory["action_history"].append(agent_action)
        self.memory["observation_history"].append(observation)
        
        console.print(Panel(
            f"[bold green]âœ… è¡ŒåŠ¨æ‰§è¡Œå®Œæˆ[/bold green]\n"
            f"[cyan]æœç´¢ç»“æœ:[/cyan] {len(search_results)} æ¡\n"
            f"[cyan]æ–°æ´å¯Ÿ:[/cyan] {len(insights)} ä¸ª\n"
            f"[cyan]ä¿¡å¿ƒåº¦:[/cyan] {confidence:.1%}",
            title="æ‰§è¡Œç»“æœ",
            border_style="green"
        ))
        
        return observation
    
    def _analyze_search_results(self, search_results: List[Dict[str, Any]], 
                              action: AgentAction) -> Tuple[List[str], List[str], float]:
        """åˆ†ææœç´¢ç»“æœï¼Œæå–æ´å¯Ÿå’Œæ–°é—®é¢˜"""
        
        if not search_results:
            return [], ["æœç´¢ç»“æœä¸ºç©ºï¼Œéœ€è¦è°ƒæ•´æœç´¢ç­–ç•¥"], 0.1
        
        # ç®€å•çš„ç»“æœåˆ†æï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPåˆ†æï¼‰
        insights = []
        new_questions = []
        
        # ç»Ÿè®¡é«˜è´¨é‡ç»“æœ
        high_quality_results = [r for r in search_results if r.get("score", 0) > 0.7]
        
        if high_quality_results:
            insights.append(f"æ‰¾åˆ° {len(high_quality_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
            
            # æå–ä¸»è¦ä¸»é¢˜
            titles = [r.get("title", "") for r in high_quality_results[:3]]
            insights.append(f"ä¸»è¦ä¸»é¢˜æ¶‰åŠ: {', '.join(titles)}")
        
        # åŸºäºç»“æœæ•°é‡å’Œè´¨é‡è®¡ç®—ä¿¡å¿ƒåº¦
        confidence = min(len(high_quality_results) / 5.0, 1.0)
        
        # ç”Ÿæˆæ–°é—®é¢˜
        if confidence < 0.6:
            new_questions.append("éœ€è¦æ›´å…·ä½“çš„æœç´¢è¯")
            new_questions.append("è€ƒè™‘ä»ä¸åŒè§’åº¦æœç´¢")
        
        return insights, new_questions, confidence
    
    def _reflecting_phase(self, observations: List[AgentObservation]) -> AgentReflection:
        """
        åæ€é˜¶æ®µï¼šè¯„ä¼°è§‚å¯Ÿç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥ç­–ç•¥
        """
        console.print(Panel(
            "[bold purple]ğŸ¤” åæ€è¯„ä¼°é˜¶æ®µ[/bold purple]",
            title=f"ç¬¬ {self.memory['current_round']} è½® - åæ€",
            border_style="purple"
        ))
        
        # æ›´æ–°ç§¯ç´¯çš„çŸ¥è¯†
        self._update_accumulated_knowledge(observations)
        
        # è¯„ä¼°æ•´ä½“è¿›åº¦
        overall_progress = self._evaluate_progress()
        
        # è¯†åˆ«çŸ¥è¯†ç¼ºå£
        knowledge_gaps = self._identify_knowledge_gaps()
        
        # å†³å®šæ˜¯å¦ç»§ç»­
        should_continue = (
            overall_progress < 0.8 and 
            self.memory["current_round"] < self.config["max_search_rounds"] and
            len(knowledge_gaps) > 0
        )
        
        reflection = AgentReflection(
            observations=observations,
            summary=f"ç¬¬{self.memory['current_round']}è½®æ”¶é›†äº†{sum(len(obs.results.get('search_results', [])) for obs in observations)}æ¡æœç´¢ç»“æœ",
            knowledge_gaps=knowledge_gaps,
            next_actions=["ç»§ç»­æ·±å…¥æœç´¢"] if should_continue else ["å‡†å¤‡æ€»ç»“ç»“è®º"],
            should_continue=should_continue,
            overall_progress=overall_progress
        )
        
        self.memory["reflection_history"].append(reflection)
        
        console.print(Panel(
            f"[bold purple]âœ… åæ€å®Œæˆ[/bold purple]\n"
            f"[cyan]æ•´ä½“è¿›åº¦:[/cyan] {overall_progress:.1%}\n"
            f"[cyan]çŸ¥è¯†ç¼ºå£:[/cyan] {len(knowledge_gaps)} ä¸ª\n"
            f"[cyan]æ˜¯å¦ç»§ç»­:[/cyan] {'æ˜¯' if should_continue else 'å¦'}",
            title="åæ€ç»“æœ",
            border_style="purple"
        ))
        
        return reflection
    
    def _update_accumulated_knowledge(self, observations: List[AgentObservation]):
        """æ›´æ–°ç§¯ç´¯çš„çŸ¥è¯†"""
        
        for obs in observations:
            search_results = obs.results.get("search_results", [])
            for result in search_results[:3]:  # åªå–å‰3ä¸ªé«˜è´¨é‡ç»“æœ
                content = result.get("content", "")[:500]  # é™åˆ¶é•¿åº¦
                if content:
                    self.memory["accumulated_knowledge"] += f"\\n\\n{content}"
        
        # é™åˆ¶æ€»çŸ¥è¯†é•¿åº¦
        if len(self.memory["accumulated_knowledge"]) > 10000:
            self.memory["accumulated_knowledge"] = self.memory["accumulated_knowledge"][-8000:]
    
    def _evaluate_progress(self) -> float:
        """è¯„ä¼°æ•´ä½“æœç´¢è¿›åº¦"""
        
        # åŸºäºè½®æ¬¡ã€çŸ¥è¯†ç§¯ç´¯é‡ã€ä¿¡å¿ƒåº¦ç­‰å› ç´ è¯„ä¼°è¿›åº¦
        round_progress = self.memory["current_round"] / self.config["max_search_rounds"]
        knowledge_progress = min(len(self.memory["accumulated_knowledge"]) / 5000, 1.0)
        
        # è®¡ç®—å¹³å‡ä¿¡å¿ƒåº¦
        recent_observations = self.memory["observation_history"][-3:]  # æœ€è¿‘3æ¬¡è§‚å¯Ÿ
        avg_confidence = 0.5
        if recent_observations:
            avg_confidence = sum(obs.confidence_score for obs in recent_observations) / len(recent_observations)
        
        overall_progress = (round_progress * 0.3 + knowledge_progress * 0.4 + avg_confidence * 0.3)
        return min(overall_progress, 1.0)
    
    def _identify_knowledge_gaps(self) -> List[str]:
        """è¯†åˆ«å½“å‰çŸ¥è¯†ç¼ºå£"""
        
        gaps = []
        
        # åŸºäºæœ€è¿‘çš„è§‚å¯Ÿè¯†åˆ«ç¼ºå£
        recent_observations = self.memory["observation_history"][-2:]
        
        for obs in recent_observations:
            if obs.confidence_score < 0.6:
                gaps.append("æœç´¢ç»“æœè´¨é‡éœ€è¦æå‡")
            
            gaps.extend(obs.new_questions[:2])  # æ·»åŠ æ–°é—®é¢˜ä½œä¸ºçŸ¥è¯†ç¼ºå£
        
        # å»é‡
        unique_gaps = list(dict.fromkeys(gaps))
        return unique_gaps[:3]  # æœ€å¤šè¿”å›3ä¸ªä¸»è¦ç¼ºå£
    
    def _finalize_results(self) -> Dict[str, Any]:
        """æ•´ç†æœ€ç»ˆæœç´¢ç»“æœ"""
        
        return {
            "original_query": self.memory["original_query"],
            "total_rounds": self.memory["current_round"],
            "final_state": self.state.value,
            "accumulated_knowledge": self.memory["accumulated_knowledge"],
            "action_history": self.memory["action_history"],
            "observation_history": self.memory["observation_history"],
            "reflection_history": self.memory["reflection_history"],
            "final_progress": self._evaluate_progress(),
            "end_time": datetime.now().isoformat()
        }
    
    def _generate_final_report(self, react_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆçš„æ·±åº¦æœç´¢æŠ¥å‘Š"""
        
        # æ”¶é›†æ‰€æœ‰æœç´¢ç»“æœ
        all_search_results = []
        for obs in react_results["observation_history"]:
            all_search_results.extend(obs.results.get("search_results", []))
        
        # ç”ŸæˆAIæ€»ç»“
        summary = self._generate_ai_summary(react_results)
        
        return {
            "query": react_results["original_query"],
            "summary": summary,
            "total_search_rounds": react_results["total_rounds"],
            "total_search_results": len(all_search_results),
            "search_results": all_search_results,
            "accumulated_knowledge": react_results["accumulated_knowledge"],
            "execution_details": {
                "actions": len(react_results["action_history"]),
                "observations": len(react_results["observation_history"]),
                "reflections": len(react_results["reflection_history"]),
                "final_progress": react_results["final_progress"]
            },
            "timestamp": react_results["end_time"]
        }
    
    def _generate_ai_summary(self, react_results: Dict[str, Any]) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæœ€ç»ˆæ€»ç»“"""
        
        summary_prompt = f"""
åŸºäºä»¥ä¸‹æ·±åº¦æœç´¢è¿‡ç¨‹å’Œç»“æœï¼Œè¯·ç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚

åŸå§‹é—®é¢˜: {react_results["original_query"]}

æœç´¢è¿‡ç¨‹æ¦‚è¿°:
- æ€»å…±è¿›è¡Œäº† {react_results["total_rounds"]} è½®ReActæœç´¢
- ç§¯ç´¯çš„çŸ¥è¯†: {len(react_results["accumulated_knowledge"])} å­—ç¬¦
- æœ€ç»ˆè¿›åº¦: {react_results["final_progress"]:.1%}

ç§¯ç´¯çš„çŸ¥è¯†å†…å®¹:
{react_results["accumulated_knowledge"][:3000]}...

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®ä¸”æœ‰ç”¨çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”åŸå§‹é—®é¢˜
2. æä¾›å…·ä½“çš„äº‹å®å’Œæ•°æ®
3. æŒ‡å‡ºä¿¡æ¯æ¥æºçš„å¯é æ€§
4. å¦‚æœ‰ä¸ç¡®å®šçš„åœ°æ–¹ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. æ€»ç»“è¦ç»“æ„æ¸…æ™°ã€é€»è¾‘è¿è´¯
"""
        
        response = self.deepseek_client.chat_completion(
            messages=[{"role": "user", "content": summary_prompt}],
            stream=False,
            temperature=0.2
        )
        
        if "error" in response:
            return f"åŸºäº {react_results['total_rounds']} è½®æ·±åº¦æœç´¢ï¼Œæ”¶é›†äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†AIæ€»ç»“ç”Ÿæˆå¤±è´¥ã€‚è¯·æŸ¥çœ‹è¯¦ç»†æœç´¢ç»“æœã€‚"
        
        return response.get("content", "æ€»ç»“ç”Ÿæˆå¤±è´¥")
    
    def _summarize_action_history(self) -> str:
        """æ€»ç»“å†å²è¡ŒåŠ¨"""
        actions = self.memory["action_history"]
        if not actions:
            return "æš‚æ— å†å²è¡ŒåŠ¨"
        
        summary = []
        for i, action in enumerate(actions[-3:], 1):  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ªè¡ŒåŠ¨
            summary.append(f"{i}. {action.action_type}: {action.reasoning[:100]}...")
        
        return "\\n".join(summary)
    
    def _summarize_recent_observations(self) -> str:
        """æ€»ç»“æœ€è¿‘çš„è§‚å¯Ÿç»“æœ"""
        observations = self.memory["observation_history"]
        if not observations:
            return "æš‚æ— è§‚å¯Ÿç»“æœ"
        
        recent_obs = observations[-2:]  # æœ€è¿‘2æ¬¡è§‚å¯Ÿ
        summary = []
        
        for obs in recent_obs:
            result_count = len(obs.results.get("search_results", []))
            summary.append(f"æœç´¢ç»“æœ: {result_count} æ¡, ä¿¡å¿ƒåº¦: {obs.confidence_score:.1%}")
        
        return "\\n".join(summary)